{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "D3HYYIkHzzun",
   "metadata": {
    "id": "D3HYYIkHzzun"
   },
   "source": [
    "# Neuroevolution on EvoGym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wL07VKpMz4e4",
   "metadata": {
    "id": "wL07VKpMz4e4"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fX-mCWU0O0y",
   "metadata": {
    "id": "4fX-mCWU0O0y"
   },
   "source": [
    "Check that the following line can run. If so, you have evogym installed! Otherwise, you need to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "AXWY14IH0OL6",
   "metadata": {
    "id": "AXWY14IH0OL6"
   },
   "outputs": [],
   "source": [
    "from evogym import sample_robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AIBLowucz6hc",
   "metadata": {
    "id": "AIBLowucz6hc"
   },
   "source": [
    "To install evogym, uncomment and run the following commands **once**. As soon as evogym is installed, comment these lines again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4aqig6T9yyYY",
   "metadata": {
    "id": "4aqig6T9yyYY"
   },
   "outputs": [],
   "source": [
    "#!git clone --recurse-submodules https://github.com/d9w/evogym.git\n",
    "#!git clone https://github.com/d9w/evolution.git\n",
    "#!sudo apt-get install xorg-dev libglu1-mesa-dev libglew-dev\n",
    "#command is python3 -m notebook\n",
    "#password is antoine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a63e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ./evogym/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28ee6b7",
   "metadata": {
    "id": "e28ee6b7"
   },
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install matplotlib\n",
    "#!pip install gym\n",
    "#!pip install tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import gym\n",
    "import evogym.envs\n",
    "from evogym import sample_robot\n",
    "from evogym.utils import get_full_connectivity\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b39cfe",
   "metadata": {
    "id": "94b39cfe"
   },
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07787020",
   "metadata": {
    "id": "07787020"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_in, h_size, n_out):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_in, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size)\n",
    "        self.fc3 = nn.Linear(h_size, n_out)\n",
    " \n",
    "        self.n_out = n_out\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00815249",
   "metadata": {
    "id": "00815249"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, Net, config, genes = None):\n",
    "        self.config = config\n",
    "        self.Net = Net\n",
    "        self.model = None\n",
    "        self.fitness = None\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.make_network()\n",
    "        if genes is not None:\n",
    "            self.genes = genes\n",
    "\n",
    "    def __repr__(self):  # pragma: no cover\n",
    "        return f\"Agent {self.model} > fitness={self.fitness}\"\n",
    "\n",
    "    def __str__(self):  # pragma: no cover\n",
    "        return self.__repr__()\n",
    "\n",
    "    def make_network(self):\n",
    "        n_in = self.config[\"n_in\"]\n",
    "        h_size = self.config[\"h_size\"]\n",
    "        n_out = self.config[\"n_out\"]\n",
    "        self.model = self.Net(n_in, h_size, n_out).to(self.device).double()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def genes(self):\n",
    "        if self.model is None:\n",
    "            return None\n",
    "        with torch.no_grad():\n",
    "            params = self.model.parameters()\n",
    "            vec = torch.nn.utils.parameters_to_vector(params)\n",
    "        return vec.cpu().double().numpy()\n",
    "\n",
    "    @genes.setter\n",
    "    def genes(self, params):\n",
    "        if self.model is None:\n",
    "            self.make_network()\n",
    "        assert len(params) == len(\n",
    "            self.genes), \"Genome size does not fit the network size\"\n",
    "        if np.isnan(params).any():\n",
    "            raise\n",
    "        a = torch.tensor(params, device=self.device)\n",
    "        torch.nn.utils.vector_to_parameters(a, self.model.parameters())\n",
    "        self.model = self.model.to(self.device).double()\n",
    "        self.fitness = None\n",
    "        return self\n",
    "\n",
    "    def mutate_ga(self):\n",
    "        genes = self.genes\n",
    "        n = len(genes)\n",
    "        f = np.random.choice([False, True], size=n, p=[1/n, 1-1/n])\n",
    "        \n",
    "        new_genes = np.empty(n)\n",
    "        new_genes[f] = genes[f]\n",
    "        noise = np.random.randn(n-sum(f))\n",
    "        new_genes[~f] = noise\n",
    "        return new_genes\n",
    "\n",
    "    def act(self, obs):\n",
    "        # continuous actions\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(obs).double().unsqueeze(0).to(self.device)\n",
    "            actions = self.model(x).cpu().detach().numpy()\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a7f25",
   "metadata": {
    "id": "f12a7f25"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19259d6",
   "metadata": {
    "id": "d19259d6"
   },
   "outputs": [],
   "source": [
    "walker = np.array([\n",
    "    [3, 3, 3, 3, 3],\n",
    "    [3, 3, 3, 0, 3],\n",
    "    [3, 3, 0, 3, 3],\n",
    "    [3, 3, 0, 3, 3],\n",
    "    [3, 3, 0, 3, 3]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d66464c",
   "metadata": {
    "id": "4d66464c"
   },
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None, robot=None, **kwargs):\n",
    "    if robot is None: \n",
    "        env = gym.make(env_name)\n",
    "    else:\n",
    "        connections = get_full_connectivity(robot)\n",
    "        env = gym.make(env_name, body=robot)\n",
    "    env.robot = robot\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a27e73",
   "metadata": {
    "id": "e5a27e73"
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, max_steps=500, render=False):\n",
    "    obs = env.reset()\n",
    "    agent.model.reset()\n",
    "    reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done and steps < max_steps:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        steps += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1881c388",
   "metadata": {
    "id": "1881c388"
   },
   "outputs": [],
   "source": [
    "def get_cfg(env_name, robot=None):\n",
    "    env = make_env(env_name, robot=walker)\n",
    "    cfg = {\n",
    "        \"n_in\": env.observation_space.shape[0],\n",
    "        \"h_size\": 32,\n",
    "        \"n_out\": env.action_space.shape[0],\n",
    "    }\n",
    "    env.close()\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72fffc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b72fffc4",
    "outputId": "2f304aca-bbf8-4a3e-e3a0-97019b67c4d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Agent Network(\n",
       "  (fc1): Linear(in_features=76, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=21, bias=True)\n",
       ") > fitness=None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Evolution Gym Simulator v2.2.5\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Walker-v0'\n",
    "robot = walker\n",
    "\n",
    "cfg = get_cfg(env_name, robot)\n",
    "a = Agent(Network, cfg)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c183d669",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c183d669",
    "outputId": "4c27837d-e9aa-4c2e-a280-3ca27b13cf36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoine/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/antoine/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/antoine/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76,\n",
       " array([ 0.        ,  0.        , -0.25135135, -0.15135135, -0.25135135,\n",
       "        -0.15135135, -0.05135135, -0.05135135,  0.04864865,  0.04864865,\n",
       "         0.14864865,  0.14864865,  0.24864865,  0.24864865, -0.25135135,\n",
       "        -0.15135135, -0.05135135,  0.04864865,  0.14864865,  0.24864865,\n",
       "        -0.25135135, -0.15135135, -0.05135135,  0.04864865,  0.04864865,\n",
       "         0.14864865,  0.24864865, -0.25135135, -0.15135135, -0.05135135,\n",
       "         0.04864865,  0.14864865,  0.24864865, -0.25135135, -0.15135135,\n",
       "        -0.05135135,  0.04864865,  0.14864865,  0.24864865,  0.24864865,\n",
       "         0.24864865,  0.14864865,  0.14864865,  0.24864865,  0.14864865,\n",
       "         0.24864865,  0.14864865,  0.24864865,  0.14864865,  0.24864865,\n",
       "         0.14864865,  0.04864865,  0.04864865,  0.04864865,  0.04864865,\n",
       "         0.04864865,  0.04864865, -0.05135135, -0.05135135, -0.05135135,\n",
       "         0.04864865, -0.05135135, -0.05135135, -0.05135135, -0.15135135,\n",
       "        -0.15135135, -0.15135135, -0.15135135, -0.15135135, -0.15135135,\n",
       "        -0.25135135, -0.25135135, -0.25135135, -0.25135135, -0.25135135,\n",
       "        -0.25135135]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_env(env_name, robot=walker)\n",
    "s = env.reset()\n",
    "len(s), s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d25dec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29d25dec",
    "outputId": "b74ac442-cfa2-402b-d516-dcb86013342f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(env_name, robot\u001b[38;5;241m=\u001b[39mwalker)\n\u001b[0;32m----> 3\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(agent, env, max_steps, render)\u001b[0m\n\u001b[1;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m---> 11\u001b[0m obs, r, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m     13\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "env = make_env(env_name, robot=walker)\n",
    "reward = evaluate(a, env, render=True)\n",
    "print(f\"Reward: {reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac629a1",
   "metadata": {
    "id": "8ac629a1"
   },
   "outputs": [],
   "source": [
    "def mp_eval(a, cfg):\n",
    "    env = make_env(cfg[\"env_name\"], robot=cfg[\"robot\"])\n",
    "    fit = evaluate(a, env, max_steps=cfg[\"max_steps\"])\n",
    "    env.close()\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f46b8a",
   "metadata": {
    "id": "34f46b8a"
   },
   "source": [
    "## Algorithms\n",
    "### 1+$\\lambda$ EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92988b5",
   "metadata": {
    "id": "f92988b5"
   },
   "outputs": [],
   "source": [
    "def one_plus_lambda(config):\n",
    "    cfg = get_cfg(config[\"env_name\"], robot=config[\"robot\"]) # Get network dims\n",
    "    cfg = {**config, **cfg} # Merge configs\n",
    "    \n",
    "    env = make_env(cfg[\"env_name\"], robot=cfg[\"robot\"])\n",
    "\n",
    "    # Elite\n",
    "    elite = Agent(Network, cfg)\n",
    "\n",
    "    elite.fitness = evaluate(elite, env, max_steps=cfg[\"max_steps\"])\n",
    "\n",
    "    fits = []\n",
    "    total_evals = []\n",
    "\n",
    "    bar = tqdm(range(cfg[\"generations\"]))\n",
    "    for gen in bar:\n",
    "        population = [Agent(Network, cfg, genes=elite.mutate_ga()) \n",
    "            for _ in range(cfg[\"lambda\"])]\n",
    "\n",
    "        with Pool(processes=len(population)) as pool:\n",
    "            pop_fitness = pool.starmap(mp_eval, [(a, cfg) for a in population])\n",
    "\n",
    "        # pop_fitness = [evaluate(a, env, max_steps=cfg[\"max_steps\"]) for a in population]\n",
    "\n",
    "        best = np.argmax(pop_fitness)\n",
    "        best_fit = pop_fitness[best]\n",
    "        if best_fit > elite.fitness:\n",
    "            elite.genes = population[best].genes\n",
    "            elite.fitness = best_fit\n",
    "        fits.append(elite.fitness)\n",
    "        total_evals.append(len(population) * (gen+1))\n",
    "        bar.set_description(f\"Best: {elite.fitness}\")\n",
    "        \n",
    "    env.close()\n",
    "    plt.plot(total_evals, fits)\n",
    "    plt.xlabel(\"Evaluations\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.show()\n",
    "    return elite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c9c12ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "4c9c12ae",
    "outputId": "78ed7b38-ca1f-4fee-979d-0dae3556609d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWalker-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot\u001b[39m\u001b[38;5;124m\"\u001b[39m: walker,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;66;03m# to change to 500\u001b[39;00m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mone_plus_lambda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m a\u001b[38;5;241m.\u001b[39mfitness\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mone_plus_lambda\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Elite\u001b[39;00m\n\u001b[1;32m      8\u001b[0m elite \u001b[38;5;241m=\u001b[39m Agent(Network, cfg)\n\u001b[0;32m---> 10\u001b[0m elite\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43melite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m fits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m total_evals \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(agent, env, max_steps, render)\u001b[0m\n\u001b[1;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m---> 11\u001b[0m obs, r, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m     13\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"env_name\": \"Walker-v0\",\n",
    "    \"robot\": walker,\n",
    "    \"generations\": 50, # To change: increase!\n",
    "    \"lambda\": 10,\n",
    "    \"max_steps\": 100, # to change to 500\n",
    "}\n",
    "\n",
    "a = one_plus_lambda(config)\n",
    "a.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614ceb85",
   "metadata": {
    "id": "614ceb85"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m make_env(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], robot\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrobot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(agent, env, max_steps, render)\u001b[0m\n\u001b[1;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m---> 11\u001b[0m obs, r, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m     13\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "env = make_env(config[\"env_name\"], robot=config[\"robot\"])\n",
    "evaluate(a, env, render=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8f67e",
   "metadata": {
    "id": "8ef8f67e"
   },
   "source": [
    "### Evolution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a660752",
   "metadata": {
    "id": "1a660752"
   },
   "outputs": [],
   "source": [
    "def ES(config):\n",
    "    cfg = get_cfg(config[\"env_name\"], robot=config[\"robot\"]) # Get network dims\n",
    "    cfg = {**config, **cfg} # Merge configs\n",
    "    \n",
    "    # Update weights\n",
    "    mu = cfg[\"mu\"]\n",
    "    w = np.array([np.log(mu + 0.5) - np.log(i)\n",
    "                          for i in range(1, mu + 1)])\n",
    "    w /= np.sum(w)\n",
    "    \n",
    "    env = make_env(cfg[\"env_name\"], robot=cfg[\"robot\"])\n",
    "\n",
    "    # Center of the distribution\n",
    "    elite = Agent(Network, cfg)\n",
    "    elite.fitness = -np.inf\n",
    "    theta = elite.genes\n",
    "    d = len(theta)\n",
    "\n",
    "    fits = []\n",
    "    total_evals = []\n",
    "\n",
    "    bar = tqdm(range(cfg[\"generations\"]))\n",
    "    for gen in bar:\n",
    "        population = []\n",
    "        for i in range(cfg[\"lambda\"]):\n",
    "            genes = theta + np.random.randn(len(theta)) * cfg[\"sigma\"]\n",
    "            ind = Agent(Network, cfg, genes=genes)\n",
    "            # ind.fitness = evaluate(ind, env, max_steps=cfg[\"max_steps\"])\n",
    "            population.append(ind)\n",
    "\n",
    "        with Pool(processes=len(population)) as pool:\n",
    "            pop_fitness = pool.starmap(mp_eval, [(a, cfg) for a in population])\n",
    "        \n",
    "        for i in range(len(population)):\n",
    "            population[i].fitness = pop_fitness[i]\n",
    "\n",
    "        # sort by fitness\n",
    "        inv_fitnesses = [- f for f in pop_fitness]\n",
    "        # indices from highest fitness to lowest\n",
    "        idx = np.argsort(inv_fitnesses)\n",
    "        \n",
    "        step = np.zeros(d)\n",
    "        for i in range(mu):\n",
    "            # update step\n",
    "            step = step + w[i] * (population[idx[i]].genes - theta)\n",
    "        # update theta\n",
    "        theta = theta + step * cfg[\"lr\"]\n",
    "\n",
    "        if pop_fitness[idx[0]] > elite.fitness:\n",
    "            elite.genes = population[idx[0]].genes\n",
    "            elite.fitness = pop_fitness[idx[0]]\n",
    "\n",
    "        fits.append(elite.fitness)\n",
    "        total_evals.append(len(population) * (gen+1))\n",
    "\n",
    "        bar.set_description(f\"Best: {elite.fitness}\")\n",
    "        \n",
    "    env.close()\n",
    "    plt.plot(total_evals, fits)\n",
    "    plt.xlabel(\"Evaluations\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.show()\n",
    "    return elite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063baf0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "063baf0f",
    "outputId": "e5682b24-34d6-4eb4-a83a-e4e7244afc21"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env_name\": \"Walker-v0\",\n",
    "    \"robot\": walker,\n",
    "    \"generations\": 50, # to change: increase!\n",
    "    \"lambda\": 10, # Population size\n",
    "    \"mu\": 5, # Parents pop size\n",
    "    \"sigma\": 0.1, # mutation std\n",
    "    \"lr\": 1, # Learning rate\n",
    "    \"max_steps\": 100, # to change to 500\n",
    "}\n",
    "\n",
    "a = ES(config)\n",
    "a.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab362318",
   "metadata": {
    "id": "ab362318"
   },
   "outputs": [],
   "source": [
    "env = make_env(config[\"env_name\"], robot=config[\"robot\"])\n",
    "evaluate(a, env, render=False)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59131bc4",
   "metadata": {
    "id": "59131bc4"
   },
   "outputs": [],
   "source": [
    "#np.save(\"walker.npy\", a.genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebcb17",
   "metadata": {
    "id": "0bebcb17"
   },
   "source": [
    "## Pre-trained walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d524b1d",
   "metadata": {
    "id": "6d524b1d"
   },
   "outputs": [],
   "source": [
    "# load weights\n",
    "\n",
    "config = {\n",
    "    \"env_name\": \"Walker-v0\",\n",
    "    \"robot\": walker,\n",
    "    \"generations\": 100,\n",
    "    \"lambda\": 10, # Population size\n",
    "    \"mu\": 5, # Parents pop size\n",
    "    \"sigma\": 0.1, # mutation std\n",
    "    \"lr\": 1, # Learning rate\n",
    "    \"max_steps\": 500,\n",
    "}\n",
    "\n",
    "cfg = get_cfg(config[\"env_name\"], robot=config[\"robot\"]) # Get network dims\n",
    "cfg = {**config, **cfg} # Merge configs\n",
    "a = Agent(Network, cfg)\n",
    "a.genes = np.load(\"walker.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c24c34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94c24c34",
    "outputId": "bc4fc5f1-4c91-46f2-8c2d-9bcdaf4ff102"
   },
   "outputs": [],
   "source": [
    "env = make_env(cfg[\"env_name\"], robot=cfg[\"robot\"])\n",
    "a.fitness = evaluate(a, env, render=False)\n",
    "env.close()\n",
    "print(a.fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe60f53",
   "metadata": {
    "id": "8fe60f53"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def save_solution(a, cfg, name=\"solution.json\"):\n",
    "    save_cfg = {}\n",
    "    for i in [\"env_name\", \"robot\", \"n_in\", \"h_size\", \"n_out\"]:\n",
    "        assert i in cfg, f\"{i} not in config\"\n",
    "        save_cfg[i] = cfg[i]\n",
    "    save_cfg[\"robot\"] = cfg[\"robot\"].tolist()\n",
    "    save_cfg[\"genes\"] = a.genes.tolist()\n",
    "    save_cfg[\"fitness\"] = float(a.fitness)\n",
    "    # save\n",
    "    with open(name, \"w\") as f:\n",
    "        json.dump(save_cfg, f)\n",
    "    return save_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea67580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_solution(a, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86ef75",
   "metadata": {
    "id": "7e86ef75"
   },
   "outputs": [],
   "source": [
    "def load_solution(name=\"solution.json\"):\n",
    "    with open(name, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "    cfg[\"robot\"] = np.array(cfg[\"robot\"])\n",
    "    cfg[\"genes\"] = np.array(cfg[\"genes\"])\n",
    "    a = Agent(Network, cfg, genes=cfg[\"genes\"])\n",
    "    a.fitness = cfg[\"fitness\"]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9e9c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ea9e9c0",
    "outputId": "7a7a75d3-4181-4bfd-d4e4-634f6b96c16d"
   },
   "outputs": [],
   "source": [
    "a = load_solution(name=\"solution.json\")\n",
    "cfg = a.config\n",
    "env = make_env(cfg[\"env_name\"], robot=cfg[\"robot\"])\n",
    "a.fitness = evaluate(a, env, render=False)\n",
    "env.close()\n",
    "print(a.fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c3f26",
   "metadata": {
    "id": "624c3f26"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "evogym.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
